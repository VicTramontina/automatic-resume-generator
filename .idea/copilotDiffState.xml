<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/.dockerignore">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/.dockerignore" />
              <option name="originalContent" value="# Ignorar arquivos desnecessários no build Docker&#10;.venv/&#10;__pycache__/&#10;*.pyc&#10;*.pyo&#10;*.pyd&#10;.Python&#10;env/&#10;venv/&#10;.env&#10;.git/&#10;.gitignore&#10;*.md&#10;.DS_Store&#10;Thumbs.db&#10;&#10;*.blg&#10;*.run.xml&#10;.pytest_cache/&#10;.coverage&#10;# Logs e arquivos temporários&#10;" />
              <option name="updatedContent" value="# Ignorar arquivos desnecessários no build Docker&#10;.venv/&#10;__pycache__/&#10;*.pyc&#10;*.pyo&#10;*.pyd&#10;.Python&#10;env/&#10;venv/&#10;.env&#10;.git/&#10;.gitignore&#10;*.md&#10;.DS_Store&#10;Thumbs.db&#10;&#10;# Arquivos de instalação local (não mais usados)&#10;pip-wheel-metadata/&#10;.tox/&#10;.coverage&#10;.pytest_cache/&#10;.mypy_cache/&#10;.dmypy.json&#10;dmypy.json&#10;&#10;# Arquivos de output que serão regenerados&#10;outputs/*&#10;!outputs/.gitkeep&#10;&#10;# Logs e arquivos temporários LaTeX&#10;*.log&#10;*.aux&#10;*.bbl&#10;*.bcf&#10;*.blg&#10;*.run.xml&#10;*.fls&#10;*.fdb_latexmk&#10;*.synctex.gz" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Dockerfile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Dockerfile" />
              <option name="originalContent" value="# Use Ubuntu as base image to handle LaTeX and system dependencies&#10;FROM ubuntu:22.04&#10;&#10;# Prevent interactive prompts during installation&#10;ENV DEBIAN_FRONTEND=noninteractive&#10;&#10;# Install system dependencies and add Google Chrome repository&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#10;    python3 \&#10;    python3-pip \&#10;    python3-venv \&#10;    texlive-latex-base \&#10;    texlive-latex-extra \&#10;    texlive-fonts-recommended \&#10;    texlive-fonts-extra \&#10;    texlive-bibtex-extra \&#10;    texlive-lang-english \&#10;    texlive-lang-portuguese \&#10;    texlive-xetex \&#10;    latexmk \&#10;    wget \&#10;    curl \&#10;    unzip \&#10;    xvfb \&#10;    gnupg \&#10;    software-properties-common \&#10;    &amp;&amp; wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \&#10;    &amp;&amp; echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt; /etc/apt/sources.list.d/google-chrome.list \&#10;    &amp;&amp; apt-get update \&#10;    &amp;&amp; apt-get install -y google-chrome-stable \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#10;&#10;# Install ChromeDriver that matches the Chrome version&#10;RUN CHROME_VERSION=$(google-chrome --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+') &amp;&amp; \&#10;    echo &quot;Chrome version: $CHROME_VERSION&quot; &amp;&amp; \&#10;    CHROME_MAJOR_VERSION=$(echo $CHROME_VERSION | cut -d. -f1) &amp;&amp; \&#10;    echo &quot;Chrome major version: $CHROME_MAJOR_VERSION&quot; &amp;&amp; \&#10;    CHROMEDRIVER_VERSION=$(curl -s &quot;https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_$CHROME_MAJOR_VERSION&quot;) &amp;&amp; \&#10;    echo &quot;Compatible ChromeDriver version: $CHROMEDRIVER_VERSION&quot; &amp;&amp; \&#10;    wget -O /tmp/chromedriver.zip &quot;https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$CHROMEDRIVER_VERSION/linux64/chromedriver-linux64.zip&quot; &amp;&amp; \&#10;    unzip /tmp/chromedriver.zip -d /tmp/ &amp;&amp; \&#10;    mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/ &amp;&amp; \&#10;    rm -rf /tmp/chromedriver.zip /tmp/chromedriver-linux64 &amp;&amp; \&#10;    chmod +x /usr/local/bin/chromedriver&#10;&#10;# Set up Chrome environment for Selenium&#10;ENV CHROME_BIN=/usr/bin/google-chrome&#10;ENV CHROME_DRIVER=/usr/local/bin/chromedriver&#10;ENV DISPLAY=:99&#10;&#10;# Create app directory&#10;WORKDIR /app&#10;&#10;# Copy requirements first to leverage Docker cache&#10;COPY requirements.txt .&#10;&#10;# Install Python dependencies&#10;RUN python3 -m pip install --upgrade pip &amp;&amp; \&#10;    python3 -m pip install -r requirements.txt&#10;&#10;# Copy the entire project&#10;COPY . .&#10;&#10;# Create outputs directory with proper permissions&#10;RUN mkdir -p outputs &amp;&amp; chmod 755 outputs&#10;&#10;# Verify Chrome and ChromeDriver installation&#10;RUN google-chrome --version &amp;&amp; \&#10;    chromedriver --version&#10;&#10;# Create a script to start Xvfb and run the application&#10;RUN echo '#!/bin/bash\n\&#10;Xvfb :99 -screen 0 1024x768x24 &gt; /dev/null 2&gt;&amp;1 &amp;\n\&#10;exec &quot;$@&quot;' &gt; /usr/local/bin/docker-entrypoint.sh &amp;&amp; \&#10;    chmod +x /usr/local/bin/docker-entrypoint.sh&#10;&#10;# Set the entrypoint&#10;ENTRYPOINT [&quot;/usr/local/bin/docker-entrypoint.sh&quot;]&#10;&#10;# Default command&#10;CMD [&quot;python3&quot;, &quot;scripts/main.py&quot;]&#10;" />
              <option name="updatedContent" value="# Use Ubuntu as base image to handle LaTeX and system dependencies&#10;FROM ubuntu:22.04&#10;&#10;# Prevent interactive prompts during installation&#10;ENV DEBIAN_FRONTEND=noninteractive&#10;&#10;# Install system dependencies and add Google Chrome repository&#10;RUN apt-get update &amp;&amp; apt-get install -y \&#10;    python3 \&#10;    python3-pip \&#10;    python3-venv \&#10;    texlive-latex-base \&#10;    texlive-latex-extra \&#10;    texlive-fonts-recommended \&#10;    texlive-fonts-extra \&#10;    texlive-bibtex-extra \&#10;    texlive-lang-english \&#10;    texlive-lang-portuguese \&#10;    texlive-xetex \&#10;    biber \&#10;    latexmk \&#10;    wget \&#10;    curl \&#10;    unzip \&#10;    xvfb \&#10;    gnupg \&#10;    software-properties-common \&#10;    &amp;&amp; wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \&#10;    &amp;&amp; echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt; /etc/apt/sources.list.d/google-chrome.list \&#10;    &amp;&amp; apt-get update \&#10;    &amp;&amp; apt-get install -y google-chrome-stable \&#10;    &amp;&amp; rm -rf /var/lib/apt/lists/*&#10;&#10;# Install ChromeDriver that matches the Chrome version&#10;RUN CHROME_VERSION=$(google-chrome --version | grep -oE '[0-9]+\.[0-9]+\.[0-9]+') &amp;&amp; \&#10;    echo &quot;Chrome version: $CHROME_VERSION&quot; &amp;&amp; \&#10;    CHROME_MAJOR_VERSION=$(echo $CHROME_VERSION | cut -d. -f1) &amp;&amp; \&#10;    echo &quot;Chrome major version: $CHROME_MAJOR_VERSION&quot; &amp;&amp; \&#10;    CHROMEDRIVER_VERSION=$(curl -s &quot;https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_$CHROME_MAJOR_VERSION&quot;) &amp;&amp; \&#10;    echo &quot;Compatible ChromeDriver version: $CHROMEDRIVER_VERSION&quot; &amp;&amp; \&#10;    wget -O /tmp/chromedriver.zip &quot;https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$CHROMEDRIVER_VERSION/linux64/chromedriver-linux64.zip&quot; &amp;&amp; \&#10;    unzip /tmp/chromedriver.zip -d /tmp/ &amp;&amp; \&#10;    mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/ &amp;&amp; \&#10;    rm -rf /tmp/chromedriver.zip /tmp/chromedriver-linux64 &amp;&amp; \&#10;    chmod +x /usr/local/bin/chromedriver&#10;&#10;# Set up Chrome environment for Selenium&#10;ENV CHROME_BIN=/usr/bin/google-chrome&#10;ENV CHROME_DRIVER=/usr/local/bin/chromedriver&#10;ENV DISPLAY=:99&#10;&#10;# Create app directory&#10;WORKDIR /app&#10;&#10;# Copy requirements first to leverage Docker cache&#10;COPY requirements.txt .&#10;&#10;# Install Python dependencies&#10;RUN python3 -m pip install --upgrade pip &amp;&amp; \&#10;    python3 -m pip install -r requirements.txt&#10;&#10;# Copy the entire project&#10;COPY . .&#10;&#10;# Create outputs directory with proper permissions&#10;RUN mkdir -p outputs &amp;&amp; chmod 755 outputs&#10;&#10;# Verify Chrome and ChromeDriver installation&#10;RUN google-chrome --version &amp;&amp; \&#10;    chromedriver --version&#10;&#10;# Create a script to start Xvfb and run the application&#10;RUN echo '#!/bin/bash\n\&#10;Xvfb :99 -screen 0 1024x768x24 &gt; /dev/null 2&gt;&amp;1 &amp;\n\&#10;exec &quot;$@&quot;' &gt; /usr/local/bin/docker-entrypoint.sh &amp;&amp; \&#10;    chmod +x /usr/local/bin/docker-entrypoint.sh&#10;&#10;# Set the entrypoint&#10;ENTRYPOINT [&quot;/usr/local/bin/docker-entrypoint.sh&quot;]&#10;&#10;# Default command&#10;CMD [&quot;python3&quot;, &quot;scripts/main.py&quot;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/Makefile">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/Makefile" />
              <option name="originalContent" value="VENV := .venv&#10;PYTHON := $(VENV)/bin/python&#10;PIP := $(VENV)/bin/pip&#10;&#10;.PHONY: setup lint run clean docker-build docker-run docker-shell docker-clean help&#10;&#10;# Comandos tradicionais (instalação local)&#10;docker-run:&#10;&#10;$(VENV)/bin/activate: requirements.txt&#10;&#9;python3 -m venv $(VENV)&#10;docker-shell:&#10;&#9;$(PIP) install -r requirements.txt&#10;&#9;touch $(VENV)/bin/activate&#10;&#10;docker-clean:&#10;&#9;$(PYTHON) -m py_compile scripts/*.py&#10;PYTHON := $(VENV)/bin/python&#10;run: lint&#10;&#9;$(PYTHON) scripts/main.py&#10;&#10;clean:&#10;&#9;rm -rf $(VENV)&#10;&#9;find outputs -mindepth 1 ! -name &quot;.gitkeep&quot; -delete&#10;&#9;@echo &quot; Comandos disponíveis:&quot;&#10;# Comandos Docker (recomendados)&#10;&#9;@echo &quot;DOCKER (Recomendado - sem necessidade de instalar dependências):&quot;&#10;&#9;@echo &quot;  make docker-build  - Constrói a imagem Docker&quot;&#10;&#9;@echo &quot;  make docker-run    - Executa o gerador de currículos&quot;&#10;&#9;@echo &quot;INSTALAÇÃO LOCAL (requer Python, LaTeX, Chrome, etc.):&quot;&#10;&#9;@echo &quot;  make setup         - Configura ambiente virtual Python&quot;&#10;&#9;@echo &quot;  make lint          - Verifica sintaxe dos scripts Python&quot;&#10;&#9;@echo &quot;  make run           - Executa o gerador de currículos&quot;&#10;&#9;@echo &quot;  make clean         - Remove ambiente virtual e outputs&quot;&#10;&#9;$(PIP) install -r requirements.txt&#10;&#9;touch $(VENV)/bin/activate&#10;&#10;lint: $(VENV)/bin/activate&#10;&#9;$(PYTHON) -m py_compile scripts/*.py&#10;&#9;@echo &quot;Para mais informações sobre Docker: ./docker-run.sh help&quot;&#10;&#10;clean:&#10;&#9;rm -rf $(VENV)&#10;&#9;find outputs -mindepth 1 ! -name &quot;.gitkeep&quot; -delete&#10;&#10;# Comandos Docker (recomendados)&#10;docker-build:&#10;&#9;@echo &quot; Construindo imagem Docker...&quot;&#10;&#9;./docker-run.sh build&#10;&#10;docker-run:&#10;&#9;@echo &quot; Executando gerador de currículos com Docker...&quot;&#10;&#9;./docker-run.sh run&#10;&#10;docker-shell:&#10;&#9;@echo &quot; Abrindo shell interativo no container...&quot;&#10;&#9;./docker-run.sh shell&#10;&#10;docker-clean:&#10;&#9;@echo &quot; Limpando containers e imagens Docker...&quot;&#10;&#9;./docker-run.sh clean&#10;&#10;# Ajuda&#10;help:&#10;&#9;@echo &quot; Comandos disponíveis:&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;DOCKER (Recomendado - sem necessidade de instalar dependências):&quot;&#10;&#9;@echo &quot;  make docker-build  - Constrói a imagem Docker&quot;&#10;&#9;@echo &quot;  make docker-run    - Executa o gerador de currículos&quot;&#10;&#9;@echo &quot;  make docker-shell  - Abre shell interativo no container&quot;&#10;&#9;@echo &quot;  make docker-clean  - Remove containers e imagens Docker&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;INSTALAÇÃO LOCAL (requer Python, LaTeX, Chrome, etc.):&quot;&#10;&#9;@echo &quot;  make setup         - Configura ambiente virtual Python&quot;&#10;&#9;@echo &quot;  make lint          - Verifica sintaxe dos scripts Python&quot;&#10;&#9;@echo &quot;  make run           - Executa o gerador de currículos&quot;&#10;&#9;@echo &quot;  make clean         - Remove ambiente virtual e outputs&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;Para mais informações sobre Docker: ./docker-run.sh help&quot;&#10;" />
              <option name="updatedContent" value=".PHONY: build run shell clean help&#10;&#10;# Comandos Docker&#10;build:&#10;&#9;@echo &quot; Construindo imagem Docker...&quot;&#10;&#9;./docker-run.sh build&#10;&#10;run:&#10;&#9;@echo &quot; Executando gerador de currículos com Docker...&quot;&#10;&#9;./docker-run.sh run&#10;&#10;shell:&#10;&#9;@echo &quot; Abrindo shell interativo no container...&quot;&#10;&#9;./docker-run.sh shell&#10;&#10;clean:&#10;&#9;@echo &quot; Limpando containers e imagens Docker...&quot;&#10;&#9;./docker-run.sh clean&#10;&#10;# Comando padrão&#10;all: run&#10;&#10;# Ajuda&#10;help:&#10;&#9;@echo &quot; Gerador de Currículos - Comandos Docker&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;COMANDOS DISPONÍVEIS:&quot;&#10;&#9;@echo &quot;  make build   - Constrói a imagem Docker&quot;&#10;&#9;@echo &quot;  make run     - Executa o gerador de currículos&quot;&#10;&#9;@echo &quot;  make shell   - Abre shell interativo no container&quot;&#10;&#9;@echo &quot;  make clean   - Remove containers e imagens Docker&quot;&#10;&#9;@echo &quot;  make help    - Mostra esta ajuda&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;EXEMPLO DE USO:&quot;&#10;&#9;@echo &quot;  make build &amp;&amp; make run&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot;Para comandos avançados: ./docker-run.sh help&quot;&#10;&#9;@echo &quot;&quot;&#10;&#9;@echo &quot; Este projeto usa exclusivamente Docker para eliminar&quot;&#10;&#9;@echo &quot;   problemas de dependências entre sistemas operacionais.&quot;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/README-DOCKER.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README-DOCKER.md" />
              <option name="updatedContent" value="# Currículo Automático&#10;&#10;Este projeto automatiza a busca de vagas remotas, a personalização de um currículo em LaTeX e a geração de PDFs específicos para cada vaga encontrada. O fluxo geral é:&#10;&#10;1. **Coletar vagas** definidas em `config/job_config.yaml`.&#10;2. **Analisar** cada vaga com a API do Gemini para adaptar o currículo.&#10;3. **Gerar saída** em `outputs/` contendo um resumo (`job.md`) e o PDF do currículo ajustado para cada vaga.&#10;&#10;##  Uso com Docker (Recomendado)&#10;&#10;**Vantagem**: Não precisa instalar nada no seu computador além do Docker! Todas as dependências (Python, LaTeX, Chrome, etc.) são gerenciadas automaticamente.&#10;&#10;### Pré-requisitos&#10;- [Docker](https://docs.docker.com/get-docker/)&#10;- [Docker Compose](https://docs.docker.com/compose/install/)&#10;&#10;### Uso Rápido&#10;&#10;1. **Configure sua API key do Gemini**:&#10;   ```bash&#10;   export GEMINI_API_KEY=&quot;sua_chave_aqui&quot;&#10;   ```&#10;&#10;2. **Construa e execute**:&#10;   ```bash&#10;   # Constrói a imagem Docker&#10;   make docker-build&#10;   &#10;   # Executa o gerador de currículos&#10;   make docker-run&#10;   ```&#10;&#10;### Comandos Docker Disponíveis&#10;&#10;```bash&#10;make docker-build   # Constrói a imagem Docker&#10;make docker-run     # Executa o gerador de currículos&#10;make docker-shell   # Abre shell interativo no container&#10;make docker-clean   # Remove containers e imagens Docker&#10;make help          # Mostra todos os comandos&#10;```&#10;&#10;### Script Docker Alternativo&#10;&#10;Você também pode usar o script `docker-run.sh` diretamente:&#10;&#10;```bash&#10;./docker-run.sh build                    # Constrói a imagem&#10;./docker-run.sh run                      # Executa o gerador&#10;./docker-run.sh shell                    # Abre shell interativo&#10;./docker-run.sh python3 scripts/main.py # Executa comando customizado&#10;./docker-run.sh help                     # Mostra ajuda detalhada&#10;```&#10;&#10;##  Instalação Local (Alternativa)&#10;&#10;**Aviso**: Requer instalar Python, LaTeX, Chrome e outras dependências manualmente.&#10;&#10;### Dependências do Sistema&#10;- Python 3.8+&#10;- LaTeX (texlive-full recomendado)&#10;- Google Chrome ou Chromium&#10;- ChromeDriver&#10;&#10;### Ubuntu/Debian&#10;```bash&#10;sudo apt update&#10;sudo apt install python3 python3-pip python3-venv texlive-full chromium-browser chromium-chromedriver&#10;```&#10;&#10;### Comandos de Instalação Local&#10;```bash&#10;make setup   # cria ambiente virtual e instala dependências&#10;make lint    # verifica sintaxe dos scripts Python&#10;make run     # roda a pipeline completa&#10;make clean   # remove ambiente virtual e saídas geradas&#10;```&#10;&#10;##  Configuração&#10;&#10;### API do Gemini&#10;1. Obtenha uma chave da API em [Google AI Studio](https://makersuite.google.com/app/apikey)&#10;2. Configure a variável de ambiente:&#10;   ```bash&#10;   export GEMINI_API_KEY=&quot;sua_chave_aqui&quot;&#10;   ```&#10;&#10;### Configuração de Vagas&#10;Edite `config/job_config.yaml` para definir os sites e critérios de busca de vagas.&#10;&#10;##  Organização do projeto&#10;&#10;```&#10;├── config/          # Arquivos de configuração&#10;│   └── job_config.yaml&#10;├── latex/           # Modelo base do currículo em LaTeX&#10;├── outputs/         # Resultados gerados (um diretório por vaga)&#10;├── scripts/         # Scripts Python de automação&#10;├── Dockerfile       # Definição da imagem Docker&#10;├── docker-compose.yml # Configuração Docker Compose&#10;├── docker-run.sh    # Script conveniente para Docker&#10;└── Makefile         # Alvos para instalação, lint e execução&#10;```&#10;&#10;##  Como Funciona&#10;&#10;### Funcionalidades de Web Scraping&#10;&#10;1. **Extração de Páginas Individuais de Vagas**: O sistema pode seguir links das vagas para extrair informações mais detalhadas.&#10;&#10;2. **Navegação por Páginas (Paginação)**: Suporta três tipos:&#10;   - **`next_button`**: Segue botões &quot;próxima página&quot;&#10;   - **`numbered_links`**: Extrai links numerados de paginação  &#10;   - **`url_pattern`**: Gera URLs baseado em padrão (ex: `?page={page}`)&#10;&#10;3. **Melhorias de Robustez**:&#10;   - Headers User-Agent realistas&#10;   - Tratamento de erros robusto&#10;   - Logs detalhados do progresso&#10;   - Rate limiting automático (1s entre páginas, 0.5s entre vagas)&#10;&#10;### Pipeline de Geração&#10;&#10;1. **Scraping**: Coleta vagas dos sites configurados&#10;2. **Análise**: Usa Gemini AI para personalizar o currículo&#10;3. **Compilação**: Gera PDF com LaTeX&#10;4. **Organização**: Salva tudo em `outputs/job_X/`&#10;&#10;##  Solução de Problemas&#10;&#10;### Problemas Comuns&#10;&#10;**Erro de dependências**: Use Docker para evitar problemas de compatibilidade entre sistemas.&#10;&#10;**Erro de LaTeX**: Verifique se todas as fontes estão disponíveis no container.&#10;&#10;**Selenium/Chrome**: O Docker já configura automaticamente um ambiente virtual com Xvfb.&#10;&#10;### Debug&#10;&#10;Para debugar problemas, use o shell interativo:&#10;```bash&#10;make docker-shell&#10;# ou&#10;./docker-run.sh shell&#10;```&#10;&#10;##  Contribuindo&#10;&#10;1. Use Docker para desenvolvimento consistente&#10;2. Teste suas mudanças com `make docker-build &amp;&amp; make docker-run`&#10;3. Mantenha a documentação atualizada&#10;&#10;##  Vantagens da Solução Docker&#10;&#10;✅ **Zero configuração**: Não precisa instalar dependências  &#10;✅ **Multiplataforma**: Funciona em Windows, macOS, Linux  &#10;✅ **Isolamento**: Não interfere com seu sistema  &#10;✅ **Reproduzível**: Mesmo ambiente para todos  &#10;✅ **Fácil limpeza**: `make docker-clean` remove tudo" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/README_NEW.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README_NEW.md" />
              <option name="updatedContent" value="# Currículo Automático&#10;&#10;Este projeto automatiza a busca de vagas remotas, a personalização de um currículo em LaTeX e a geração de PDFs específicos para cada vaga encontrada. O fluxo geral é:&#10;&#10;1. **Coletar vagas** definidas em `config/job_config.yaml`.&#10;2. **Analisar** cada vaga com a API do Gemini para adaptar o currículo.&#10;3. **Gerar saída** em `outputs/` contendo um resumo (`job.md`) e o PDF do currículo ajustado para cada vaga.&#10;&#10;## Organização do projeto&#10;&#10;```&#10;├── config/          # Arquivos de configuração&#10;│   └── job_config.yaml&#10;├── latex/           # Modelo base do currículo em LaTeX&#10;├── outputs/         # Resultados gerados (um diretório por vaga)&#10;├── scripts/         # Scripts Python de automação&#10;└── Makefile         # Alvos para instalação, lint e execução&#10;```&#10;&#10;## Usando o Makefile&#10;&#10;```bash&#10;make setup   # cria ambiente virtual e instala dependências&#10;make lint    # verifica sintaxe dos scripts Python&#10;make run     # roda a pipeline completa&#10;make clean   # remove ambiente virtual e saídas geradas&#10;```&#10;&#10;Para personalizar o currículo, defina a variável de ambiente `GEMINI_API_KEY` com sua chave da API do Gemini antes de executar `make run`.&#10;&#10;## Funcionalidades Avançadas de Web Scraping&#10;&#10;### 1. Extração de Páginas Individuais de Vagas&#10;&#10;O sistema agora pode seguir links das vagas para extrair informações mais detalhadas das páginas individuais.&#10;&#10;### 2. Navegação por Páginas (Paginação)&#10;&#10;O sistema suporta três tipos de paginação:&#10;&#10;- **`next_button`**: Segue botões &quot;próxima página&quot; até não existir mais&#10;- **`numbered_links`**: Extrai links numerados de paginação  &#10;- **`url_pattern`**: Gera URLs baseado em um padrão (ex: `?page={page}`)&#10;&#10;### 3. Melhorias de Robustez&#10;&#10;- Headers de User-Agent realistas para evitar bloqueios&#10;- Tratamento de erros que permite continuar mesmo se algumas páginas falharem&#10;- Logs detalhados do progresso do scraping&#10;- Rate limiting automático entre requisições (1s entre páginas, 0.5s entre vagas)&#10;&#10;## Configuração das vagas (`job_config.yaml`)&#10;&#10;O arquivo `config/job_config.yaml` controla tanto o que será buscado quanto os filtros aplicados às vagas. Os principais parâmetros são:&#10;&#10;- `skills`: lista de palavras-chave que devem aparecer na vaga. Se nenhuma estiver presente, a vaga é ignorada.&#10;- `salary`:&#10;  - `usd`: salário mínimo em dólares (USD).&#10;  - `brl`: salário mínimo em reais (BRL).&#10;- `sites`: lista de sites a serem raspados. Cada site contém:&#10;  - `name`: nome do site.&#10;  - `url`: endereço da página de vagas.&#10;  - `job_selector`: seletor CSS que identifica cada card de vaga na página.&#10;  - `fields`: mapeamento de campos desejados para seletores CSS relativos ao card:&#10;    - `title`: título da vaga&#10;    - `company`: nome da empresa&#10;    - `link`: **OBRIGATÓRIO** - link para a página individual da vaga&#10;  - `detail_fields`: campos extraídos da página individual da vaga:&#10;    - `description`: descrição completa&#10;    - `salary`: informações de salário&#10;    - `skills`: tecnologias/habilidades&#10;    - `requirements`: requisitos&#10;    - `benefits`: benefícios&#10;  - `pagination`: configuração para navegar por múltiplas páginas:&#10;    - `type`: tipo de paginação (`next_button`, `numbered_links`, ou `url_pattern`)&#10;    - `max_pages`: máximo de páginas a percorrer&#10;    - Para `next_button`: `next_selector` (seletor do botão próxima)&#10;    - Para `numbered_links`: `links_selector` (seletor dos links de página)&#10;    - Para `url_pattern`: `url_pattern` (padrão da URL com `{page}`)&#10;&#10;### Exemplo de Configuração Completa&#10;&#10;```yaml&#10;skills:&#10;  - Python&#10;  - JavaScript&#10;  - Docker&#10;salary:&#10;  usd: 3000&#10;  brl: 6000&#10;&#10;sites:&#10;  - name: &quot;Remotar&quot;&#10;    url: &quot;https://remotar.com.br/search/jobs?q=desenvolvedor&quot;&#10;    job_selector: &quot;div.job-card&quot;&#10;    &#10;    # Campos básicos da listagem&#10;    fields:&#10;      title: &quot;h2&quot;&#10;      company: &quot;.company&quot;&#10;      link: &quot;a&quot;  # Link para página individual&#10;    &#10;    # Campos detalhados da página individual&#10;    detail_fields:&#10;      description: &quot;.job-description&quot;&#10;      salary: &quot;.salary-info&quot;&#10;      skills: &quot;.required-skills .tag&quot;&#10;      requirements: &quot;.requirements&quot;&#10;      benefits: &quot;.benefits&quot;&#10;    &#10;    # Configuração de paginação&#10;    pagination:&#10;      type: &quot;next_button&quot;&#10;      next_selector: &quot;.pagination .next&quot;&#10;      max_pages: 5&#10;```&#10;&#10;### Tipos de Paginação&#10;&#10;#### Botão &quot;Próxima&quot;&#10;```yaml&#10;pagination:&#10;  type: &quot;next_button&quot;&#10;  next_selector: &quot;.pagination .next&quot;&#10;  max_pages: 5&#10;```&#10;&#10;#### Links Numerados&#10;```yaml&#10;pagination:&#10;  type: &quot;numbered_links&quot;&#10;  links_selector: &quot;.pagination a&quot;&#10;  max_pages: 10&#10;```&#10;&#10;#### Padrão de URL&#10;```yaml&#10;pagination:&#10;  type: &quot;url_pattern&quot;&#10;  url_pattern: &quot;https://site.com/jobs?page={page}&quot;&#10;  max_pages: 5&#10;```&#10;&#10;Adapte estes campos conforme a estrutura HTML do site que deseja consultar.&#10;&#10;## Saídas&#10;&#10;Para cada vaga aprovada pelos filtros, é criado um diretório `outputs/job_N/` contendo:&#10;&#10;- `job.md`: resumo em Markdown com os dados coletados.&#10;- `resume.tex`: currículo ajustado em LaTeX.&#10;- `resume.pdf`: currículo compilado.&#10;&#10;Assim, você obtém um PDF personalizado para cada vaga remota filtrada.&#10;&#10;## Logs e Monitoramento&#10;&#10;O sistema fornece logs detalhados durante a execução:&#10;&#10;```&#10;Scraping Remotar...&#10;Found 2 additional pages to scrape&#10;Scraping page: https://remotar.com.br/search/jobs?q=desenvolvedor&#10;Found 10 job listings on this page&#10;Scraping details for: Desenvolvedor Python Sênior&#10;Scraping details for: Desenvolvedor Frontend React&#10;...&#10;Total jobs found: 25&#10;```&#10;&#10;## Dicas para Configuração&#10;&#10;1. **Teste os seletores**: Use as ferramentas de desenvolvedor do navegador para testar os seletores CSS&#10;2. **URLs relativas**: O sistema automaticamente converte URLs relativas em absolutas&#10;3. **Campos opcionais**: Se um campo não for encontrado, será definido como `None`&#10;4. **Paginação cautelosa**: Comece com `max_pages` baixo para testar&#10;5. **Rate limiting**: O sistema já inclui delays automáticos para ser respeitoso com os servidores" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/SCRAPING_GUIDE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/SCRAPING_GUIDE.md" />
              <option name="updatedContent" value="# Guia de Web Scraping Avançado&#10;&#10;Este guia explica como usar as funcionalidades avançadas do web scraping implementadas no projeto.&#10;&#10;## Funcionalidades Implementadas&#10;&#10;### 1. Extração de Páginas Individuais de Vagas&#10;&#10;O sistema agora pode seguir links das vagas para extrair informações mais detalhadas das páginas individuais.&#10;&#10;#### Como Configurar:&#10;&#10;```yaml&#10;sites:&#10;  - name: &quot;Nome do Site&quot;&#10;    url: &quot;URL da página de listagem&quot;&#10;    job_selector: &quot;seletor CSS para cada vaga&quot;&#10;    fields:&#10;      title: &quot;seletor para título&quot;&#10;      company: &quot;seletor para empresa&quot;&#10;      link: &quot;seletor para o link da vaga&quot;  # IMPORTANTE: Campo obrigatório&#10;    detail_fields:  # Campos extraídos da página individual&#10;      description: &quot;seletor para descrição completa&quot;&#10;      salary: &quot;seletor para salário&quot;&#10;      skills: &quot;seletor para skills&quot;&#10;      requirements: &quot;seletor para requisitos&quot;&#10;      benefits: &quot;seletor para benefícios&quot;&#10;```&#10;&#10;### 2. Navegação por Páginas (Paginação)&#10;&#10;O sistema suporta três tipos de paginação:&#10;&#10;#### Tipo 1: Botão &quot;Próxima&quot;&#10;```yaml&#10;pagination:&#10;  type: &quot;next_button&quot;&#10;  next_selector: &quot;.pagination .next&quot;  # Seletor do botão &quot;próxima&quot;&#10;  max_pages: 5  # Máximo de páginas a percorrer&#10;```&#10;&#10;#### Tipo 2: Links Numerados&#10;```yaml&#10;pagination:&#10;  type: &quot;numbered_links&quot;&#10;  links_selector: &quot;.pagination a&quot;  # Seletor dos links de página&#10;  max_pages: 10&#10;```&#10;&#10;#### Tipo 3: Padrão de URL&#10;```yaml&#10;pagination:&#10;  type: &quot;url_pattern&quot;&#10;  url_pattern: &quot;https://site.com/jobs?page={page}&quot;  # {page} será substituído&#10;  max_pages: 5&#10;```&#10;&#10;## Exemplo de Configuração Completa&#10;&#10;```yaml&#10;skills:&#10;  - Python&#10;  - JavaScript&#10;  - Docker&#10;salary:&#10;  usd: 3000&#10;  brl: 6000&#10;&#10;sites:&#10;  - name: &quot;Site Exemplo&quot;&#10;    url: &quot;https://exemplo.com/vagas&quot;&#10;    job_selector: &quot;.vaga-card&quot;&#10;    &#10;    # Campos básicos da listagem&#10;    fields:&#10;      title: &quot;h2.titulo&quot;&#10;      company: &quot;.empresa&quot;&#10;      link: &quot;a.ver-mais&quot;  # Link para página individual&#10;    &#10;    # Campos detalhados da página individual&#10;    detail_fields:&#10;      description: &quot;.descricao-completa&quot;&#10;      salary: &quot;.salario&quot;&#10;      skills: &quot;.tecnologias .tag&quot;&#10;      requirements: &quot;.requisitos&quot;&#10;      benefits: &quot;.beneficios&quot;&#10;    &#10;    # Configuração de paginação&#10;    pagination:&#10;      type: &quot;next_button&quot;&#10;      next_selector: &quot;.proxima-pagina&quot;&#10;      max_pages: 3&#10;```&#10;&#10;## Funcionalidades de Segurança&#10;&#10;- **Rate Limiting**: Delay automático entre requisições (1 segundo entre páginas, 0.5 segundo entre vagas individuais)&#10;- **User-Agent**: Headers realistas para evitar bloqueios&#10;- **Error Handling**: Tratamento de erros para continuar o scraping mesmo se algumas páginas falharem&#10;- **Timeout**: Timeout de 30 segundos por requisição&#10;&#10;## Logs e Monitoramento&#10;&#10;O sistema agora fornece logs detalhados:&#10;- Número de páginas encontradas&#10;- Progresso do scraping por página&#10;- Erros encontrados&#10;- Total de vagas coletadas&#10;&#10;## Dicas para Configuração&#10;&#10;1. **Teste os seletores**: Use as ferramentas de desenvolvedor do navegador para testar os seletores CSS&#10;2. **URLs relativas**: O sistema automaticamente converte URLs relativas em absolutas&#10;3. **Campos opcionais**: Se um campo não for encontrado, será definido como `None`&#10;4. **Paginação cautelosa**: Comece com `max_pages` baixo para testar&#10;&#10;## Exemplo de Execução&#10;&#10;```bash&#10;cd scripts&#10;python scrape_jobs.py --config ../config/job_config.yaml&#10;```&#10;&#10;O output mostrará o progresso:&#10;```&#10;Scraping Remotar...&#10;Found 2 additional pages to scrape&#10;Scraping page: https://remotar.com.br/search/jobs?q=desenvolvedor&#10;Found 10 job listings on this page&#10;Scraping details for: Desenvolvedor Python Sênior&#10;Scraping details for: Desenvolvedor Frontend React&#10;...&#10;Total jobs found: 25&#10;```" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/docker-compose.yml">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/docker-compose.yml" />
              <option name="originalContent" value="&#10;" />
              <option name="updatedContent" value="version: '3.8'&#10;&#10;services:&#10;  resume-generator:&#10;    build: .&#10;    container_name: curriculo-generator&#10;    volumes:&#10;      # Mount the outputs directory to persist generated resumes&#10;      - ./outputs:/app/outputs&#10;      # Mount config directory for easy configuration changes&#10;      - ./config:/app/config&#10;    environment:&#10;      - DISPLAY=:99&#10;      - GEMINI_API_KEY=${GEMINI_API_KEY}&#10;    # Keep container running for interactive use&#10;    stdin_open: true&#10;    tty: true" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/latex/resume.tex">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/latex/resume.tex" />
              <option name="originalContent" value="%-------------------------------------------------------------------------------&#10;% CONFIGURATION&#10;%-------------------------------------------------------------------------------&#10;&#10;\documentclass[11pt, a4paper]{russell}&#10;&#10;\usepackage{xcolor}&#10;&#10;\geometry{left=1.4cm, top=.8cm, right=1.4cm, bottom=1.8cm, footskip=.5cm}&#10;&#10;\fontdir[fonts/]&#10;&#10;\colorlet{russell}{russell-black}&#10;&#10;\setbool{acvSectionColorHighlight}{true}&#10;&#10;\renewcommand{\acvHeaderSocialSep}{\quad\textbar\quad}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% PERSONAL INFORMATION&#10;%-------------------------------------------------------------------------------&#10;&#10;\name{Victor Augusto}{Tramontina}&#10;&#10;\position{Computer Engineering -- \textbf{Full Stack Developer}}&#10;&#10;\mobile{+55 (54) 9 9937-7877}&#10;&#10;\email{victor.tramontina@universo.univates.br}&#10;&#10;\github{https://github.com/VicTramontina}&#10;&#10;% \linkedin{linkedin.com/in/victor-augusto-tramontina-ba3b26299}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;\begin{document}&#10;&#10;    \makecvheader&#10;&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;&#10;&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;&#10;&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;&#10;&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;    \makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% SUMMARY&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{About Me}&#10;&#10;    \begin{cvparagraph}&#10;        \textbf{Motivated and collaborative full stack developer} with strong problem-solving skills and a solid background in \textbf{backend (Vanilla PHP, SQL)} and \textbf{modern frontend (ReactJS, TailwindCSS, Styled Components)}. Experienced in maintaining legacy systems, refactoring architectures, and integrating \textbf{cloud services (AWS)} and third-party APIs (OAuth2). Frequently leverages \textbf{AI tools and assistants} to support coding, debugging, and technical decision-making. Enthusiastic about learning and sharing knowledge, with an active role in \textbf{team discussions and technology decisions}.&#10;    \end{cvparagraph}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% SKILLS&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Skills}&#10;&#10;    \begin{cvskills}&#10;        \cvskill{Frontend}{ReactJS, TailwindCSS, Styled Components, React Hook Form, Zod}&#10;        \cvskill{Backend}{Vanilla PHP, Node, SQL, REST APIs, OAuth2}&#10;        \cvskill{DevOps \&amp; Tools}{Docker, Git, AWS (EC2, S3, Lambda, Cognito, Amplify), Linux, Crontabs}&#10;        \cvskill{Mobile}{React Native, Java, Android Studio, SQLite}&#10;        \cvskill{Integration}{Payment gateways, ERP platforms, Notifications and e-mail systems}&#10;    \end{cvskills}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% EXPERIENCE&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Professional Experience}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {\textbf{Full Stack Developer}}&#10;        {Macro Publicidade -- Conecta Venda System}&#10;        {Brazil}&#10;        {2022 -- Present}&#10;        {&#10;            \begin{cvitems}&#10;                \item {Worked in a \textbf{small agile team} (5 members) using \textbf{Scrum methodology}.}&#10;                \item {Maintained \textbf{legacy PHP systems} and refactored old AngularJS frontends into \textbf{ReactJS}.}&#10;                \item {Helped architect and implement \textbf{system-wide improvements} in frontend and database models.}&#10;                \item {Developed \textbf{reusable frontend components} and backend utilities to streamline workflows.}&#10;                \item {Participated in \textbf{technical decision-making}, mentoring junior teammates, and improving team practices.}&#10;                \item {Integrated external services using \textbf{OAuth2}, including payment gateways and ERP systems.}&#10;                \item {Handled \textbf{deployments and environment management} using Docker and AWS.}&#10;                \item {Contributed to the digital transformation of the company's systems, improving maintainability and scalability.}&#10;            \end{cvitems}&#10;        }&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% PROJECTS&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Personal Projects}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {Guaporé - RS}&#10;        {Android Ticketing System}&#10;        {2023}&#10;        {}&#10;        {&#10;            \begin{cvitems}&#10;                \item {Developed a \textbf{complete Android app} for ticket printing and bar sales management.}&#10;                \item {Implemented product registration, order creation and report exports.}&#10;                \item {System is used by \textbf{over 8 companies} in the city.}&#10;                \item {Technologies used: \textbf{Java, Android Studio, SQLite, Epson SDK}.}&#10;            \end{cvitems}&#10;        }&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% EDUCATION&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Education}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {Computer Engineering}&#10;        {Univates}&#10;        {Brazil}&#10;        {2022 -- Present, \textbf{7th Semester}}&#10;        {}&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% LANGUAGES&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Languages}&#10;&#10;    \begin{cvskills}&#10;        \cvskill{Portuguese}{\textbf{Native}}&#10;        \cvskill{English}{\textbf{Advanced}}&#10;    \end{cvskills}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;\end{document}&#10;" />
              <option name="updatedContent" value="%-------------------------------------------------------------------------------&#10;% CONFIGURATION&#10;%-------------------------------------------------------------------------------&#10;&#10;\documentclass[11pt, a4paper]{russell}&#10;&#10;\usepackage{xcolor}&#10;&#10;\geometry{left=1.4cm, top=.8cm, right=1.4cm, bottom=1.8cm, footskip=.5cm}&#10;&#10;\fontdir[fonts/]&#10;&#10;\colorlet{russell}{russell-black}&#10;&#10;\setbool{acvSectionColorHighlight}{true}&#10;&#10;\renewcommand{\acvHeaderSocialSep}{\quad\textbar\quad}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% PERSONAL INFORMATION&#10;%-------------------------------------------------------------------------------&#10;&#10;\name{Victor Augusto}{Tramontina}&#10;&#10;\position{Computer Engineering -- \textbf{Full Stack Developer}}&#10;&#10;\mobile{+55 (54) 9 9937-7877}&#10;&#10;\email{victor.tramontina@universo.univates.br}&#10;&#10;\github{https://github.com/VicTramontina}&#10;&#10;% \linkedin{linkedin.com/in/victor-augusto-tramontina-ba3b26299}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;\begin{document}&#10;&#10;\makecvheader&#10;\makecvfooter{}{Victor Augusto Tramontina}{\thepage}&#10;&#10;%-------------------------------------------------------------------------------&#10;% SUMMARY&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{About Me}&#10;&#10;    \begin{cvparagraph}&#10;        \textbf{Motivated and collaborative full stack developer} with strong problem-solving skills and a solid background in \textbf{backend (Vanilla PHP, SQL)} and \textbf{modern frontend (ReactJS, TailwindCSS, Styled Components)}. Experienced in maintaining legacy systems, refactoring architectures, and integrating \textbf{cloud services (AWS)} and third-party APIs (OAuth2). Frequently leverages \textbf{AI tools and assistants} to support coding, debugging, and technical decision-making. Enthusiastic about learning and sharing knowledge, with an active role in \textbf{team discussions and technology decisions}.&#10;    \end{cvparagraph}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% SKILLS&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Skills}&#10;&#10;    \begin{cvskills}&#10;        \cvskill{Frontend}{ReactJS, TailwindCSS, Styled Components, React Hook Form, Zod}&#10;        \cvskill{Backend}{Vanilla PHP, Node, SQL, REST APIs, OAuth2}&#10;        \cvskill{DevOps \&amp; Tools}{Docker, Git, AWS (EC2, S3, Lambda, Cognito, Amplify), Linux, Crontabs}&#10;        \cvskill{Mobile}{React Native, Java, Android Studio, SQLite}&#10;        \cvskill{Integration}{Payment gateways, ERP platforms, Notifications and e-mail systems}&#10;    \end{cvskills}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% EXPERIENCE&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Professional Experience}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {\textbf{Full Stack Developer}}&#10;        {Macro Publicidade -- Conecta Venda System}&#10;        {Brazil}&#10;        {2022 -- Present}&#10;        {&#10;            \begin{cvitems}&#10;                \item {Worked in a \textbf{small agile team} (5 members) using \textbf{Scrum methodology}.}&#10;                \item {Maintained \textbf{legacy PHP systems} and refactored old AngularJS frontends into \textbf{ReactJS}.}&#10;                \item {Helped architect and implement \textbf{system-wide improvements} in frontend and database models.}&#10;                \item {Developed \textbf{reusable frontend components} and backend utilities to streamline workflows.}&#10;                \item {Participated in \textbf{technical decision-making}, mentoring junior teammates, and improving team practices.}&#10;                \item {Integrated external services using \textbf{OAuth2}, including payment gateways and ERP systems.}&#10;                \item {Handled \textbf{deployments and environment management} using Docker and AWS.}&#10;                \item {Contributed to the digital transformation of the company's systems, improving maintainability and scalability.}&#10;            \end{cvitems}&#10;        }&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% PROJECTS&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Personal Projects}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {Guaporé - RS}&#10;        {Android Ticketing System}&#10;        {2023}&#10;        {}&#10;        {&#10;            \begin{cvitems}&#10;                \item {Developed a \textbf{complete Android app} for ticket printing and bar sales management.}&#10;                \item {Implemented product registration, order creation and report exports.}&#10;                \item {System is used by \textbf{over 8 companies} in the city.}&#10;                \item {Technologies used: \textbf{Java, Android Studio, SQLite, Epson SDK}.}&#10;            \end{cvitems}&#10;        }&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% EDUCATION&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Education}&#10;&#10;    \begin{cventries}&#10;        \cventry&#10;        {Computer Engineering}&#10;        {Univates}&#10;        {Brazil}&#10;        {2022 -- Present, \textbf{7th Semester}}&#10;        {}&#10;    \end{cventries}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;% LANGUAGES&#10;%-------------------------------------------------------------------------------&#10;&#10;    \cvsection{Languages}&#10;&#10;    \begin{cvskills}&#10;        \cvskill{Portuguese}{\textbf{Native}}&#10;        \cvskill{English}{\textbf{Advanced}}&#10;    \end{cvskills}&#10;&#10;&#10;%-------------------------------------------------------------------------------&#10;\end{document}" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/scripts/scrape_jobs.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scripts/scrape_jobs.py" />
              <option name="originalContent" value="&quot;&quot;&quot;Enhanced web scraping utilities for job listings.&#10;&#10;Reads configuration from a YAML file describing the web sites to scrape and&#10;returns a list of job dictionaries with the desired fields extracted using&#10;CSS selectors. Supports individual job page extraction, pagination, and infinite scroll.&#10;&quot;&quot;&quot;&#10;&#10;from __future__ import annotations&#10;&#10;import re&#10;import time&#10;from urllib.parse import urljoin, urlparse&#10;&#10;import yaml&#10;import requests&#10;from bs4 import BeautifulSoup&#10;from pathlib import Path&#10;from typing import List, Dict, Any, Optional&#10;&#10;# Selenium imports for infinite scroll&#10;try:&#10;    from selenium import webdriver&#10;    from selenium.webdriver.chrome.service import Service&#10;    from selenium.webdriver.chrome.options import Options&#10;    from selenium.webdriver.common.by import By&#10;    from selenium.webdriver.support.ui import WebDriverWait&#10;    from selenium.webdriver.support import expected_conditions as EC&#10;    from selenium.common.exceptions import TimeoutException, NoSuchElementException&#10;    from webdriver_manager.chrome import ChromeDriverManager&#10;    SELENIUM_AVAILABLE = True&#10;except ImportError:&#10;    SELENIUM_AVAILABLE = False&#10;&#10;&#10;def _parse_salary(text: str) -&gt; tuple[str, float]:&#10;    &quot;&quot;&quot;Return currency code and amount extracted from *text*.&quot;&quot;&quot;&#10;    if not text:&#10;        return &quot;&quot;, 0.0&#10;    currency = &quot;&quot;&#10;    if &quot;R$&quot; in text or &quot;BRL&quot; in text:&#10;        currency = &quot;BRL&quot;&#10;    elif &quot;$&quot; in text or &quot;USD&quot; in text:&#10;        currency = &quot;USD&quot;&#10;    match = re.search(r&quot;(\d+[\.,]?\d*)&quot;, text)&#10;    if not match:&#10;        return currency, 0.0&#10;    amount = float(match.group(1).replace(&quot;.&quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;.&quot;))&#10;    return currency, amount&#10;&#10;&#10;def _get_page_content(url: str, headers: Optional[Dict[str, str]] = None) -&gt; BeautifulSoup:&#10;    &quot;&quot;&quot;Get and parse page content with error handling.&quot;&quot;&quot;&#10;    default_headers = {&#10;        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'&#10;    }&#10;    if headers:&#10;        default_headers.update(headers)&#10;&#10;    response = requests.get(url, headers=default_headers, timeout=30)&#10;    response.raise_for_status()&#10;    return BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;&#10;&#10;def _extract_job_data(soup: BeautifulSoup, fields: Dict[str, str], base_url: str = &quot;&quot;) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;Extract job data from a soup object using field selectors.&quot;&quot;&quot;&#10;    job: Dict[str, Any] = {}&#10;    for field, selector in fields.items():&#10;        target = soup.select_one(selector)&#10;        if target:&#10;            if field == &quot;link&quot; and target.get(&quot;href&quot;):&#10;                # Handle relative URLs&#10;                job[field] = urljoin(base_url, target.get(&quot;href&quot;))&#10;            else:&#10;                job[field] = target.get_text(strip=True)&#10;        else:&#10;            job[field] = None&#10;    return job&#10;&#10;&#10;def _scrape_individual_job(job_url: str, detail_fields: Dict[str, str]) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;Scrape detailed information from an individual job page.&quot;&quot;&quot;&#10;    try:&#10;        soup = _get_page_content(job_url)&#10;        return _extract_job_data(soup, detail_fields, job_url)&#10;    except Exception as e:&#10;        print(f&quot;Error scraping job details from {job_url}: {e}&quot;)&#10;        return {}&#10;&#10;&#10;def _check_skills_match(job_text: str, skills_config: List[Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Check if job matches skill requirements based on configuration.&quot;&quot;&quot;&#10;    job_text_lower = job_text.lower()&#10;&#10;    for skill_entry in skills_config:&#10;        if isinstance(skill_entry, str):&#10;            # Old format: just a string (optional skill)&#10;            if skill_entry.lower() in job_text_lower:&#10;                return True&#10;        elif isinstance(skill_entry, dict):&#10;            # New format: dict with skill and required flag&#10;            skill_name = skill_entry.get(&quot;name&quot;, &quot;&quot;).lower()&#10;            is_required = skill_entry.get(&quot;required&quot;, False)&#10;&#10;            skill_found = skill_name in job_text_lower&#10;&#10;            if is_required and not skill_found:&#10;                # Required skill not found, job doesn't match&#10;                return False&#10;            elif not is_required and skill_found:&#10;                # Optional skill found, job matches&#10;                return True&#10;&#10;    # If we have any required skills, at least one must be found&#10;    has_required_skills = any(&#10;        isinstance(skill, dict) and skill.get(&quot;required&quot;, False)&#10;        for skill in skills_config&#10;    )&#10;&#10;    if has_required_skills:&#10;        # Check if at least one required skill was found&#10;        for skill_entry in skills_config:&#10;            if isinstance(skill_entry, dict) and skill_entry.get(&quot;required&quot;, False):&#10;                if skill_entry.get(&quot;name&quot;, &quot;&quot;).lower() in job_text_lower:&#10;                    return True&#10;        return False&#10;&#10;    # If no required skills and no optional skills found, check old format&#10;    return any(&#10;        isinstance(skill, str) and skill.lower() in job_text_lower&#10;        for skill in skills_config&#10;    )&#10;&#10;&#10;def _get_next_page_url(soup: BeautifulSoup, base_url: str, pagination_config: Dict[str, Any]) -&gt; Optional[str]:&#10;    &quot;&quot;&quot;Get the next page URL based on pagination configuration.&quot;&quot;&quot;&#10;    if pagination_config.get(&quot;type&quot;) == &quot;next_button&quot;:&#10;        next_selector = pagination_config.get(&quot;next_selector&quot;, &quot;&quot;)&#10;        next_link = soup.select_one(next_selector)&#10;        if next_link and next_link.get(&quot;href&quot;):&#10;            return urljoin(base_url, next_link.get(&quot;href&quot;))&#10;&#10;    elif pagination_config.get(&quot;type&quot;) == &quot;numbered_links&quot;:&#10;        # For numbered links, we need to track current page and get next&#10;        links_selector = pagination_config.get(&quot;links_selector&quot;, &quot;&quot;)&#10;        page_links = soup.select(links_selector)&#10;&#10;        # Find current active page and get next one&#10;        for i, link in enumerate(page_links):&#10;            if link.get(&quot;aria-current&quot;) == &quot;page&quot; or &quot;current&quot; in link.get(&quot;class&quot;, []):&#10;                # Found current page, return next if exists&#10;                if i + 1 &lt; len(page_links):&#10;                    next_link = page_links[i + 1]&#10;                    if next_link.get(&quot;href&quot;):&#10;                        return urljoin(base_url, next_link.get(&quot;href&quot;))&#10;                break&#10;&#10;    return None&#10;&#10;&#10;def _setup_selenium_driver() -&gt; Optional[webdriver.Chrome]:&#10;    &quot;&quot;&quot;Setup Chrome driver for infinite scroll support.&quot;&quot;&quot;&#10;    if not SELENIUM_AVAILABLE:&#10;        print(&quot;Selenium not available. Install with: pip install selenium webdriver-manager&quot;)&#10;        return None&#10;&#10;    try:&#10;        print(&quot;Setting up Chrome driver for Docker environment...&quot;)&#10;&#10;        # Check if Chrome and ChromeDriver are available&#10;        import subprocess&#10;        try:&#10;            chrome_version = subprocess.run([&quot;/usr/bin/google-chrome&quot;, &quot;--version&quot;],&#10;                                          capture_output=True, text=True, check=True)&#10;            print(f&quot;Chrome version: {chrome_version.stdout.strip()}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Chrome binary not found: {e}&quot;)&#10;            return None&#10;&#10;        try:&#10;            driver_version = subprocess.run([&quot;/usr/local/bin/chromedriver&quot;, &quot;--version&quot;],&#10;                                          capture_output=True, text=True, check=True)&#10;            print(f&quot;ChromeDriver version: {driver_version.stdout.strip()}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;ChromeDriver binary not found: {e}&quot;)&#10;            return None&#10;&#10;        options = Options()&#10;        options.add_argument(&quot;--headless=new&quot;)  # Use new headless mode&#10;        options.add_argument(&quot;--no-sandbox&quot;)&#10;        options.add_argument(&quot;--disable-dev-shm-usage&quot;)&#10;        options.add_argument(&quot;--disable-gpu&quot;)&#10;        options.add_argument(&quot;--disable-extensions&quot;)&#10;        options.add_argument(&quot;--disable-plugins&quot;)&#10;        options.add_argument(&quot;--disable-images&quot;)&#10;        options.add_argument(&quot;--disable-background-timer-throttling&quot;)&#10;        options.add_argument(&quot;--disable-backgrounding-occluded-windows&quot;)&#10;        options.add_argument(&quot;--disable-renderer-backgrounding&quot;)&#10;        options.add_argument(&quot;--disable-features=TranslateUI&quot;)&#10;        options.add_argument(&quot;--disable-ipc-flooding-protection&quot;)&#10;        options.add_argument(&quot;--disable-background-networking&quot;)&#10;        options.add_argument(&quot;--disable-default-apps&quot;)&#10;        options.add_argument(&quot;--disable-sync&quot;)&#10;        options.add_argument(&quot;--disable-translate&quot;)&#10;        options.add_argument(&quot;--hide-scrollbars&quot;)&#10;        options.add_argument(&quot;--metrics-recording-only&quot;)&#10;        options.add_argument(&quot;--mute-audio&quot;)&#10;        options.add_argument(&quot;--no-first-run&quot;)&#10;        options.add_argument(&quot;--safebrowsing-disable-auto-update&quot;)&#10;        options.add_argument(&quot;--disable-crash-reporter&quot;)&#10;        options.add_argument(&quot;--disable-logging&quot;)&#10;        options.add_argument(&quot;--disable-permissions-api&quot;)&#10;        options.add_argument(&quot;--window-size=1920,1080&quot;)&#10;        options.add_argument(&quot;--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36&quot;)&#10;        &#10;        # Enhanced Docker-specific options for better stability&#10;        options.add_argument(&quot;--remote-debugging-port=9222&quot;)&#10;        options.add_argument(&quot;--disable-web-security&quot;)&#10;        options.add_argument(&quot;--allow-running-insecure-content&quot;)&#10;        options.add_argument(&quot;--disable-features=VizDisplayCompositor,AudioServiceOutOfProcess&quot;)&#10;        options.add_argument(&quot;--disable-software-rasterizer&quot;)&#10;        options.add_argument(&quot;--disable-background-networking&quot;)&#10;        options.add_argument(&quot;--disable-background-timer-throttling&quot;)&#10;        options.add_argument(&quot;--disable-renderer-backgrounding&quot;)&#10;        options.add_argument(&quot;--disable-backgrounding-occluded-windows&quot;)&#10;        options.add_argument(&quot;--disable-client-side-phishing-detection&quot;)&#10;        options.add_argument(&quot;--disable-default-apps&quot;)&#10;        options.add_argument(&quot;--disable-hang-monitor&quot;)&#10;        options.add_argument(&quot;--disable-popup-blocking&quot;)&#10;        options.add_argument(&quot;--disable-prompt-on-repost&quot;)&#10;        options.add_argument(&quot;--disable-sync&quot;)&#10;        options.add_argument(&quot;--force-color-profile=srgb&quot;)&#10;        options.add_argument(&quot;--memory-pressure-off&quot;)&#10;        options.add_argument(&quot;--max_old_space_size=4096&quot;)&#10;        options.add_argument(&quot;--no-zygote&quot;)  # Important for Docker&#10;        options.add_argument(&quot;--single-process&quot;)  # Run in single process mode for containers&#10;        &#10;        # Use system Google Chrome binary&#10;        options.binary_location = &quot;/usr/bin/google-chrome&quot;&#10;&#10;        # Use system chromedriver&#10;        service = Service(&quot;/usr/local/bin/chromedriver&quot;)&#10;&#10;        print(&quot;Attempting to create Chrome driver instance...&quot;)&#10;        driver = webdriver.Chrome(service=service, options=options)&#10;        print(&quot;✅ Chrome driver successfully created!&quot;)&#10;        return driver&#10;    except Exception as e:&#10;        print(f&quot;Failed to setup Chrome driver: {e}&quot;)&#10;        print(f&quot;Error type: {type(e).__name__}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return None&#10;&#10;&#10;def _scrape_with_infinite_scroll(site: Dict[str, Any], max_jobs: int) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Scrape jobs from a site that uses infinite scroll (like LinkedIn).&quot;&quot;&quot;&#10;    if not SELENIUM_AVAILABLE:&#10;        print(&quot;Selenium not available for infinite scroll. Falling back to regular scraping.&quot;)&#10;        return []&#10;&#10;    driver = _setup_selenium_driver()&#10;    if not driver:&#10;        return []&#10;&#10;    try:&#10;        url = site[&quot;url&quot;]&#10;        print(f&quot;Loading {url} with Selenium for infinite scroll...&quot;)&#10;        driver.get(url)&#10;&#10;        # Wait for initial content to load&#10;        time.sleep(3)&#10;&#10;        jobs = []&#10;        last_height = 0&#10;        scroll_attempts = 0&#10;        max_scroll_attempts = site.get(&quot;max_scrolls&quot;, 10)&#10;&#10;        while len(jobs) &lt; max_jobs and scroll_attempts &lt; max_scroll_attempts:&#10;            # Get current page content&#10;            soup = BeautifulSoup(driver.page_source, &quot;html.parser&quot;)&#10;            job_elements = soup.select(site.get(&quot;job_selector&quot;, &quot;&quot;))&#10;&#10;            print(f&quot;Scroll {scroll_attempts + 1}: Found {len(job_elements)} total job listings&quot;)&#10;&#10;            # Process new jobs (skip already processed ones)&#10;            for i, elem in enumerate(job_elements[len(jobs):]):&#10;                if len(jobs) &gt;= max_jobs:&#10;                    break&#10;&#10;                job_soup = BeautifulSoup(str(elem), &quot;html.parser&quot;)&#10;                job = _extract_job_data(job_soup, site.get(&quot;fields&quot;, {}), url)&#10;&#10;                # Skip if no link found&#10;                if not job.get(&quot;link&quot;):&#10;                    continue&#10;&#10;                # Extract individual job details if configured&#10;                if job.get(&quot;link&quot;) and site.get(&quot;detail_fields&quot;):&#10;                    print(f&quot;Scraping details for: {job.get('title', 'Unknown title')}&quot;)&#10;                    detailed_job = _scrape_individual_job(job[&quot;link&quot;], site[&quot;detail_fields&quot;])&#10;                    job.update(detailed_job)&#10;                    time.sleep(0.5)&#10;&#10;                jobs.append(job)&#10;                print(f&quot;✓ Job {len(jobs)}/{max_jobs} collected: {job.get('title', 'Unknown title')}&quot;)&#10;&#10;            # Scroll down to load more content&#10;            driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;)&#10;            time.sleep(2)&#10;&#10;            # Check if we've reached the bottom or no new content loaded&#10;            new_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;)&#10;            if new_height == last_height:&#10;                print(&quot;No more content to load&quot;)&#10;                break&#10;&#10;            last_height = new_height&#10;            scroll_attempts += 1&#10;&#10;        print(f&quot;Collected {len(jobs)} jobs with infinite scroll&quot;)&#10;        return jobs&#10;&#10;    except Exception as e:&#10;        print(f&quot;Error during infinite scroll scraping: {e}&quot;)&#10;        return []&#10;    finally:&#10;        driver.quit()&#10;&#10;&#10;def scrape_jobs(config_path: str | Path) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Scrape job listings defined in *config_path*.&#10;&#10;    The configuration must contain a ``sites`` list and may include ``skills``&#10;    keywords as well as ``salary`` thresholds in USD or BRL.&#10;&#10;    Enhanced to support individual job page extraction, pagination, and infinite scroll.&#10;    &quot;&quot;&quot;&#10;    config_path = Path(config_path)&#10;    with config_path.open(&quot;r&quot;, encoding=&quot;utf-8&quot;) as fh:&#10;        config = yaml.safe_load(fh)&#10;&#10;    skills_config = config.get(&quot;skills&quot;, [])&#10;    salary_cfg = config.get(&quot;salary&quot;, {})&#10;    min_usd = salary_cfg.get(&quot;usd&quot;)&#10;    min_brl = salary_cfg.get(&quot;brl&quot;)&#10;&#10;    jobs: List[Dict[str, Any]] = []&#10;&#10;    for site in config.get(&quot;sites&quot;, []):&#10;        print(f&quot;Scraping {site.get('name', 'Unknown site')}...&quot;)&#10;&#10;        max_jobs = site.get(&quot;max_jobs&quot;, 50)  # Default to 50 jobs per site&#10;&#10;        # Check if this site uses infinite scroll&#10;        if site.get(&quot;pagination&quot;, {}).get(&quot;type&quot;) == &quot;infinite_scroll&quot;:&#10;            print(f&quot;Using infinite scroll for {site.get('name')}&quot;)&#10;            site_jobs = _scrape_with_infinite_scroll(site, max_jobs)&#10;&#10;            # Apply filters to scraped jobs&#10;            filtered_jobs = []&#10;            for job in site_jobs:&#10;                # Apply skill filters&#10;                haystack = &quot; &quot;.join(&#10;                    filter(None, [job.get(&quot;skills&quot;), job.get(&quot;description&quot;), job.get(&quot;title&quot;)])&#10;                )&#10;&#10;                if skills_config and not _check_skills_match(haystack, skills_config):&#10;                    continue&#10;&#10;                # Apply salary filters&#10;                cur, amount = _parse_salary(job.get(&quot;salary&quot;, &quot;&quot;))&#10;                if (&#10;                    (cur == &quot;USD&quot; and min_usd and amount &lt; min_usd)&#10;                    or (cur == &quot;BRL&quot; and min_brl and amount &lt; min_brl)&#10;                ):&#10;                    continue&#10;&#10;                filtered_jobs.append(job)&#10;                if len(filtered_jobs) &gt;= max_jobs:&#10;                    break&#10;&#10;            jobs.extend(filtered_jobs)&#10;            print(f&quot;Total filtered jobs from {site.get('name')}: {len(filtered_jobs)}&quot;)&#10;            continue&#10;&#10;        # Regular pagination scraping (existing logic)&#10;        site_jobs = []&#10;        current_url = site[&quot;url&quot;]&#10;        page_count = 0&#10;        max_pages = site.get(&quot;max_pages&quot;, 20)  # Safety limit to prevent infinite loops&#10;&#10;        while len(site_jobs) &lt; max_jobs and page_count &lt; max_pages:&#10;            page_count += 1&#10;&#10;            try:&#10;                print(f&quot;Scraping page {page_count}: {current_url}&quot;)&#10;                soup = _get_page_content(current_url)&#10;&#10;                # Extract job listings from current page&#10;                job_elements = soup.select(site.get(&quot;job_selector&quot;, &quot;&quot;))&#10;                print(f&quot;Found {len(job_elements)} job listings on this page&quot;)&#10;&#10;                page_jobs_added = 0&#10;                for elem in job_elements:&#10;                    if len(site_jobs) &gt;= max_jobs:&#10;                        print(f&quot;Reached maximum jobs limit ({max_jobs}) for {site.get('name')}&quot;)&#10;                        break&#10;&#10;                    # Extract basic job info and link&#10;                    job_soup = BeautifulSoup(str(elem), &quot;html.parser&quot;)&#10;                    job = _extract_job_data(job_soup, site.get(&quot;fields&quot;, {}), current_url)&#10;&#10;                    # If we have a job link and detail fields, scrape the individual page&#10;                    if job.get(&quot;link&quot;) and site.get(&quot;detail_fields&quot;):&#10;                        print(f&quot;Scraping details for: {job.get('title', 'Unknown title')}&quot;)&#10;                        detailed_job = _scrape_individual_job(job[&quot;link&quot;], site[&quot;detail_fields&quot;])&#10;                        job.update(detailed_job)  # Merge detailed info&#10;                        time.sleep(0.5)  # Be respectful to the server&#10;&#10;                    # Apply skill filters&#10;                    haystack = &quot; &quot;.join(&#10;                        filter(None, [job.get(&quot;skills&quot;), job.get(&quot;description&quot;), job.get(&quot;title&quot;)])&#10;                    )&#10;&#10;                    if skills_config and not _check_skills_match(haystack, skills_config):&#10;                        continue&#10;&#10;                    # Apply salary filters&#10;                    cur, amount = _parse_salary(job.get(&quot;salary&quot;, &quot;&quot;))&#10;                    if (&#10;                        (cur == &quot;USD&quot; and min_usd and amount &lt; min_usd)&#10;                        or (cur == &quot;BRL&quot; and min_brl and amount &lt; min_brl)&#10;                    ):&#10;                        continue&#10;&#10;                    site_jobs.append(job)&#10;                    page_jobs_added += 1&#10;                    print(f&quot;✓ Job {len(site_jobs)}/{max_jobs} added: {job.get('title', 'Unknown title')}&quot;)&#10;&#10;                print(f&quot;Added {page_jobs_added} jobs from this page&quot;)&#10;&#10;                # Check if we should continue to next page&#10;                if len(site_jobs) &gt;= max_jobs:&#10;                    print(f&quot;Reached target of {max_jobs} jobs for {site.get('name')}&quot;)&#10;                    break&#10;&#10;                # Get next page URL if pagination is configured&#10;                if &quot;pagination&quot; in site:&#10;                    next_url = _get_next_page_url(soup, current_url, site[&quot;pagination&quot;])&#10;                    if next_url and next_url != current_url:&#10;                        current_url = next_url&#10;                        time.sleep(1)  # Be respectful between pages&#10;                    else:&#10;                        print(&quot;No more pages available&quot;)&#10;                        break&#10;                else:&#10;                    # No pagination configured, stop after first page&#10;                    break&#10;&#10;            except Exception as e:&#10;                print(f&quot;Error scraping page {current_url}: {e}&quot;)&#10;                break&#10;&#10;        jobs.extend(site_jobs)&#10;        print(f&quot;Total jobs collected from {site.get('name')}: {len(site_jobs)}&quot;)&#10;&#10;    print(f&quot;Total jobs found across all sites: {len(jobs)}&quot;)&#10;    return jobs&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    import argparse, json&#10;&#10;    parser = argparse.ArgumentParser(description=&quot;Scrape job listings&quot;)&#10;    parser.add_argument(&quot;--config&quot;, default=&quot;../config/job_config.yaml&quot;)&#10;    args = parser.parse_args()&#10;    results = scrape_jobs(args.config)&#10;    print(json.dumps(results, indent=2, ensure_ascii=False))&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;Enhanced web scraping utilities for job listings.&#10;&#10;Reads configuration from a YAML file describing the web sites to scrape and&#10;returns a list of job dictionaries with the desired fields extracted using&#10;CSS selectors. Supports individual job page extraction, pagination, and infinite scroll.&#10;&quot;&quot;&quot;&#10;&#10;from __future__ import annotations&#10;&#10;import re&#10;import time&#10;from urllib.parse import urljoin, urlparse&#10;&#10;import yaml&#10;import requests&#10;from bs4 import BeautifulSoup&#10;from pathlib import Path&#10;from typing import List, Dict, Any, Optional&#10;&#10;# Selenium imports for infinite scroll&#10;try:&#10;    from selenium import webdriver&#10;    from selenium.webdriver.chrome.service import Service&#10;    from selenium.webdriver.chrome.options import Options&#10;    from selenium.webdriver.common.by import By&#10;    from selenium.webdriver.support.ui import WebDriverWait&#10;    from selenium.webdriver.support import expected_conditions as EC&#10;    from selenium.common.exceptions import TimeoutException, NoSuchElementException&#10;    from webdriver_manager.chrome import ChromeDriverManager&#10;    SELENIUM_AVAILABLE = True&#10;except ImportError:&#10;    SELENIUM_AVAILABLE = False&#10;&#10;&#10;def _parse_salary(text: str) -&gt; tuple[str, float]:&#10;    &quot;&quot;&quot;Return currency code and amount extracted from *text*.&quot;&quot;&quot;&#10;    if not text:&#10;        return &quot;&quot;, 0.0&#10;    currency = &quot;&quot;&#10;    if &quot;R$&quot; in text or &quot;BRL&quot; in text:&#10;        currency = &quot;BRL&quot;&#10;    elif &quot;$&quot; in text or &quot;USD&quot; in text:&#10;        currency = &quot;USD&quot;&#10;    match = re.search(r&quot;(\d+[\.,]?\d*)&quot;, text)&#10;    if not match:&#10;        return currency, 0.0&#10;    amount = float(match.group(1).replace(&quot;.&quot;, &quot;&quot;).replace(&quot;,&quot;, &quot;.&quot;))&#10;    return currency, amount&#10;&#10;&#10;def _get_page_content(url: str, headers: Optional[Dict[str, str]] = None) -&gt; BeautifulSoup:&#10;    &quot;&quot;&quot;Get and parse page content with error handling.&quot;&quot;&quot;&#10;    default_headers = {&#10;        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'&#10;    }&#10;    if headers:&#10;        default_headers.update(headers)&#10;&#10;    response = requests.get(url, headers=default_headers, timeout=30)&#10;    response.raise_for_status()&#10;    return BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;&#10;&#10;def _extract_job_data(soup: BeautifulSoup, fields: Dict[str, str], base_url: str = &quot;&quot;) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;Extract job data from a soup object using field selectors.&quot;&quot;&quot;&#10;    job: Dict[str, Any] = {}&#10;    for field, selector in fields.items():&#10;        target = soup.select_one(selector)&#10;        if target:&#10;            if field == &quot;link&quot; and target.get(&quot;href&quot;):&#10;                # Handle relative URLs&#10;                job[field] = urljoin(base_url, target.get(&quot;href&quot;))&#10;            else:&#10;                job[field] = target.get_text(strip=True)&#10;        else:&#10;            job[field] = None&#10;    return job&#10;&#10;&#10;def _scrape_individual_job(job_url: str, detail_fields: Dict[str, str]) -&gt; Dict[str, Any]:&#10;    &quot;&quot;&quot;Scrape detailed information from an individual job page.&quot;&quot;&quot;&#10;    try:&#10;        soup = _get_page_content(job_url)&#10;        return _extract_job_data(soup, detail_fields, job_url)&#10;    except Exception as e:&#10;        print(f&quot;Error scraping job details from {job_url}: {e}&quot;)&#10;        return {}&#10;&#10;&#10;def _check_skills_match(job_text: str, skills_config: List[Any]) -&gt; bool:&#10;    &quot;&quot;&quot;Check if job matches skill requirements based on configuration.&quot;&quot;&quot;&#10;    job_text_lower = job_text.lower()&#10;&#10;    for skill_entry in skills_config:&#10;        if isinstance(skill_entry, str):&#10;            # Old format: just a string (optional skill)&#10;            if skill_entry.lower() in job_text_lower:&#10;                return True&#10;        elif isinstance(skill_entry, dict):&#10;            # New format: dict with skill and required flag&#10;            skill_name = skill_entry.get(&quot;name&quot;, &quot;&quot;).lower()&#10;            is_required = skill_entry.get(&quot;required&quot;, False)&#10;&#10;            skill_found = skill_name in job_text_lower&#10;&#10;            if is_required and not skill_found:&#10;                # Required skill not found, job doesn't match&#10;                return False&#10;            elif not is_required and skill_found:&#10;                # Optional skill found, job matches&#10;                return True&#10;&#10;    # If we have any required skills, at least one must be found&#10;    has_required_skills = any(&#10;        isinstance(skill, dict) and skill.get(&quot;required&quot;, False)&#10;        for skill in skills_config&#10;    )&#10;&#10;    if has_required_skills:&#10;        # Check if at least one required skill was found&#10;        for skill_entry in skills_config:&#10;            if isinstance(skill_entry, dict) and skill_entry.get(&quot;required&quot;, False):&#10;                if skill_entry.get(&quot;name&quot;, &quot;&quot;).lower() in job_text_lower:&#10;                    return True&#10;        return False&#10;&#10;    # If no required skills and no optional skills found, check old format&#10;    return any(&#10;        isinstance(skill, str) and skill.lower() in job_text_lower&#10;        for skill in skills_config&#10;    )&#10;&#10;&#10;def _get_next_page_url(soup: BeautifulSoup, base_url: str, pagination_config: Dict[str, Any]) -&gt; Optional[str]:&#10;    &quot;&quot;&quot;Get the next page URL based on pagination configuration.&quot;&quot;&quot;&#10;    if pagination_config.get(&quot;type&quot;) == &quot;next_button&quot;:&#10;        next_selector = pagination_config.get(&quot;next_selector&quot;, &quot;&quot;)&#10;        next_link = soup.select_one(next_selector)&#10;        if next_link and next_link.get(&quot;href&quot;):&#10;            return urljoin(base_url, next_link.get(&quot;href&quot;))&#10;&#10;    elif pagination_config.get(&quot;type&quot;) == &quot;numbered_links&quot;:&#10;        # For numbered links, we need to track current page and get next&#10;        links_selector = pagination_config.get(&quot;links_selector&quot;, &quot;&quot;)&#10;        page_links = soup.select(links_selector)&#10;&#10;        # Find current active page and get next one&#10;        for i, link in enumerate(page_links):&#10;            if link.get(&quot;aria-current&quot;) == &quot;page&quot; or &quot;current&quot; in link.get(&quot;class&quot;, []):&#10;                # Found current page, return next if exists&#10;                if i + 1 &lt; len(page_links):&#10;                    next_link = page_links[i + 1]&#10;                    if next_link.get(&quot;href&quot;):&#10;                        return urljoin(base_url, next_link.get(&quot;href&quot;))&#10;                break&#10;&#10;    return None&#10;&#10;&#10;def _setup_selenium_driver() -&gt; Optional[webdriver.Chrome]:&#10;    &quot;&quot;&quot;Setup Chrome driver for infinite scroll support.&quot;&quot;&quot;&#10;    if not SELENIUM_AVAILABLE:&#10;        print(&quot;Selenium not available. Install with: pip install selenium webdriver-manager&quot;)&#10;        return None&#10;&#10;    try:&#10;        print(&quot;Setting up Chrome driver for Docker environment...&quot;)&#10;&#10;        # Check if Chrome and ChromeDriver are available&#10;        import subprocess&#10;        try:&#10;            chrome_version = subprocess.run([&quot;/usr/bin/google-chrome&quot;, &quot;--version&quot;],&#10;                                          capture_output=True, text=True, check=True)&#10;            print(f&quot;Chrome version: {chrome_version.stdout.strip()}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Chrome binary not found: {e}&quot;)&#10;            return None&#10;&#10;        try:&#10;            driver_version = subprocess.run([&quot;/usr/local/bin/chromedriver&quot;, &quot;--version&quot;],&#10;                                          capture_output=True, text=True, check=True)&#10;            print(f&quot;ChromeDriver version: {driver_version.stdout.strip()}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;ChromeDriver binary not found: {e}&quot;)&#10;            return None&#10;&#10;        options = Options()&#10;        options.add_argument(&quot;--headless=new&quot;)  # Use new headless mode&#10;        options.add_argument(&quot;--no-sandbox&quot;)&#10;        options.add_argument(&quot;--disable-dev-shm-usage&quot;)&#10;        options.add_argument(&quot;--disable-gpu&quot;)&#10;        options.add_argument(&quot;--disable-extensions&quot;)&#10;        options.add_argument(&quot;--disable-plugins&quot;)&#10;        options.add_argument(&quot;--disable-images&quot;)&#10;        options.add_argument(&quot;--disable-background-timer-throttling&quot;)&#10;        options.add_argument(&quot;--disable-backgrounding-occluded-windows&quot;)&#10;        options.add_argument(&quot;--disable-renderer-backgrounding&quot;)&#10;        options.add_argument(&quot;--disable-features=TranslateUI,VizDisplayCompositor&quot;)&#10;        options.add_argument(&quot;--disable-ipc-flooding-protection&quot;)&#10;        options.add_argument(&quot;--disable-background-networking&quot;)&#10;        options.add_argument(&quot;--disable-default-apps&quot;)&#10;        options.add_argument(&quot;--disable-sync&quot;)&#10;        options.add_argument(&quot;--disable-translate&quot;)&#10;        options.add_argument(&quot;--hide-scrollbars&quot;)&#10;        options.add_argument(&quot;--metrics-recording-only&quot;)&#10;        options.add_argument(&quot;--mute-audio&quot;)&#10;        options.add_argument(&quot;--no-first-run&quot;)&#10;        options.add_argument(&quot;--safebrowsing-disable-auto-update&quot;)&#10;        options.add_argument(&quot;--disable-crash-reporter&quot;)&#10;        options.add_argument(&quot;--disable-logging&quot;)&#10;        options.add_argument(&quot;--disable-permissions-api&quot;)&#10;        options.add_argument(&quot;--window-size=1920,1080&quot;)&#10;        options.add_argument(&quot;--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36&quot;)&#10;        &#10;        # Simplified Docker-specific options&#10;        options.add_argument(&quot;--disable-web-security&quot;)&#10;        options.add_argument(&quot;--allow-running-insecure-content&quot;)&#10;        options.add_argument(&quot;--disable-software-rasterizer&quot;)&#10;        options.add_argument(&quot;--disable-client-side-phishing-detection&quot;)&#10;        options.add_argument(&quot;--disable-hang-monitor&quot;)&#10;        options.add_argument(&quot;--disable-popup-blocking&quot;)&#10;        options.add_argument(&quot;--disable-prompt-on-repost&quot;)&#10;        options.add_argument(&quot;--force-color-profile=srgb&quot;)&#10;        options.add_argument(&quot;--memory-pressure-off&quot;)&#10;        options.add_argument(&quot;--no-zygote&quot;)  # Important for Docker&#10;        &#10;        # Use system Google Chrome binary&#10;        options.binary_location = &quot;/usr/bin/google-chrome&quot;&#10;&#10;        # Use system chromedriver&#10;        service = Service(&quot;/usr/local/bin/chromedriver&quot;)&#10;&#10;        print(&quot;Attempting to create Chrome driver instance...&quot;)&#10;        driver = webdriver.Chrome(service=service, options=options)&#10;        print(&quot;✅ Chrome driver successfully created!&quot;)&#10;        return driver&#10;    except Exception as e:&#10;        print(f&quot;Failed to setup Chrome driver: {e}&quot;)&#10;        print(f&quot;Error type: {type(e).__name__}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return None&#10;&#10;&#10;def _scrape_with_infinite_scroll(site: Dict[str, Any], max_jobs: int) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Scrape jobs from a site that uses infinite scroll (like LinkedIn).&quot;&quot;&quot;&#10;    if not SELENIUM_AVAILABLE:&#10;        print(&quot;Selenium not available for infinite scroll. Falling back to regular scraping.&quot;)&#10;        return []&#10;&#10;    driver = _setup_selenium_driver()&#10;    if not driver:&#10;        return []&#10;&#10;    try:&#10;        url = site[&quot;url&quot;]&#10;        print(f&quot;Loading {url} with Selenium for infinite scroll...&quot;)&#10;        driver.get(url)&#10;&#10;        # Wait for initial content to load&#10;        time.sleep(3)&#10;&#10;        jobs = []&#10;        last_height = 0&#10;        scroll_attempts = 0&#10;        max_scroll_attempts = site.get(&quot;max_scrolls&quot;, 10)&#10;&#10;        while len(jobs) &lt; max_jobs and scroll_attempts &lt; max_scroll_attempts:&#10;            # Get current page content&#10;            soup = BeautifulSoup(driver.page_source, &quot;html.parser&quot;)&#10;            job_elements = soup.select(site.get(&quot;job_selector&quot;, &quot;&quot;))&#10;&#10;            print(f&quot;Scroll {scroll_attempts + 1}: Found {len(job_elements)} total job listings&quot;)&#10;&#10;            # Process new jobs (skip already processed ones)&#10;            for i, elem in enumerate(job_elements[len(jobs):]):&#10;                if len(jobs) &gt;= max_jobs:&#10;                    break&#10;&#10;                job_soup = BeautifulSoup(str(elem), &quot;html.parser&quot;)&#10;                job = _extract_job_data(job_soup, site.get(&quot;fields&quot;, {}), url)&#10;&#10;                # Skip if no link found&#10;                if not job.get(&quot;link&quot;):&#10;                    continue&#10;&#10;                # Extract individual job details if configured&#10;                if job.get(&quot;link&quot;) and site.get(&quot;detail_fields&quot;):&#10;                    print(f&quot;Scraping details for: {job.get('title', 'Unknown title')}&quot;)&#10;                    detailed_job = _scrape_individual_job(job[&quot;link&quot;], site[&quot;detail_fields&quot;])&#10;                    job.update(detailed_job)&#10;                    time.sleep(0.5)&#10;&#10;                jobs.append(job)&#10;                print(f&quot;✓ Job {len(jobs)}/{max_jobs} collected: {job.get('title', 'Unknown title')}&quot;)&#10;&#10;            # Scroll down to load more content&#10;            driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;)&#10;            time.sleep(2)&#10;&#10;            # Check if we've reached the bottom or no new content loaded&#10;            new_height = driver.execute_script(&quot;return document.body.scrollHeight&quot;)&#10;            if new_height == last_height:&#10;                print(&quot;No more content to load&quot;)&#10;                break&#10;&#10;            last_height = new_height&#10;            scroll_attempts += 1&#10;&#10;        print(f&quot;Collected {len(jobs)} jobs with infinite scroll&quot;)&#10;        return jobs&#10;&#10;    except Exception as e:&#10;        print(f&quot;Error during infinite scroll scraping: {e}&quot;)&#10;        return []&#10;    finally:&#10;        driver.quit()&#10;&#10;&#10;def scrape_jobs(config_path: str | Path) -&gt; List[Dict[str, Any]]:&#10;    &quot;&quot;&quot;Scrape job listings defined in *config_path*.&#10;&#10;    The configuration must contain a ``sites`` list and may include ``skills``&#10;    keywords as well as ``salary`` thresholds in USD or BRL.&#10;&#10;    Enhanced to support individual job page extraction, pagination, and infinite scroll.&#10;    &quot;&quot;&quot;&#10;    config_path = Path(config_path)&#10;    with config_path.open(&quot;r&quot;, encoding=&quot;utf-8&quot;) as fh:&#10;        config = yaml.safe_load(fh)&#10;&#10;    skills_config = config.get(&quot;skills&quot;, [])&#10;    salary_cfg = config.get(&quot;salary&quot;, {})&#10;    min_usd = salary_cfg.get(&quot;usd&quot;)&#10;    min_brl = salary_cfg.get(&quot;brl&quot;)&#10;&#10;    jobs: List[Dict[str, Any]] = []&#10;&#10;    for site in config.get(&quot;sites&quot;, []):&#10;        print(f&quot;Scraping {site.get('name', 'Unknown site')}...&quot;)&#10;&#10;        max_jobs = site.get(&quot;max_jobs&quot;, 50)  # Default to 50 jobs per site&#10;&#10;        # Check if this site uses infinite scroll&#10;        if site.get(&quot;pagination&quot;, {}).get(&quot;type&quot;) == &quot;infinite_scroll&quot;:&#10;            print(f&quot;Using infinite scroll for {site.get('name')}&quot;)&#10;            site_jobs = _scrape_with_infinite_scroll(site, max_jobs)&#10;&#10;            # Apply filters to scraped jobs&#10;            filtered_jobs = []&#10;            for job in site_jobs:&#10;                # Apply skill filters&#10;                haystack = &quot; &quot;.join(&#10;                    filter(None, [job.get(&quot;skills&quot;), job.get(&quot;description&quot;), job.get(&quot;title&quot;)])&#10;                )&#10;&#10;                if skills_config and not _check_skills_match(haystack, skills_config):&#10;                    continue&#10;&#10;                # Apply salary filters&#10;                cur, amount = _parse_salary(job.get(&quot;salary&quot;, &quot;&quot;))&#10;                if (&#10;                    (cur == &quot;USD&quot; and min_usd and amount &lt; min_usd)&#10;                    or (cur == &quot;BRL&quot; and min_brl and amount &lt; min_brl)&#10;                ):&#10;                    continue&#10;&#10;                filtered_jobs.append(job)&#10;                if len(filtered_jobs) &gt;= max_jobs:&#10;                    break&#10;&#10;            jobs.extend(filtered_jobs)&#10;            print(f&quot;Total filtered jobs from {site.get('name')}: {len(filtered_jobs)}&quot;)&#10;            continue&#10;&#10;        # Regular pagination scraping (existing logic)&#10;        site_jobs = []&#10;        current_url = site[&quot;url&quot;]&#10;        page_count = 0&#10;        max_pages = site.get(&quot;max_pages&quot;, 20)  # Safety limit to prevent infinite loops&#10;&#10;        while len(site_jobs) &lt; max_jobs and page_count &lt; max_pages:&#10;            page_count += 1&#10;&#10;            try:&#10;                print(f&quot;Scraping page {page_count}: {current_url}&quot;)&#10;                soup = _get_page_content(current_url)&#10;&#10;                # Extract job listings from current page&#10;                job_elements = soup.select(site.get(&quot;job_selector&quot;, &quot;&quot;))&#10;                print(f&quot;Found {len(job_elements)} job listings on this page&quot;)&#10;&#10;                page_jobs_added = 0&#10;                for elem in job_elements:&#10;                    if len(site_jobs) &gt;= max_jobs:&#10;                        print(f&quot;Reached maximum jobs limit ({max_jobs}) for {site.get('name')}&quot;)&#10;                        break&#10;&#10;                    # Extract basic job info and link&#10;                    job_soup = BeautifulSoup(str(elem), &quot;html.parser&quot;)&#10;                    job = _extract_job_data(job_soup, site.get(&quot;fields&quot;, {}), current_url)&#10;&#10;                    # If we have a job link and detail fields, scrape the individual page&#10;                    if job.get(&quot;link&quot;) and site.get(&quot;detail_fields&quot;):&#10;                        print(f&quot;Scraping details for: {job.get('title', 'Unknown title')}&quot;)&#10;                        detailed_job = _scrape_individual_job(job[&quot;link&quot;], site[&quot;detail_fields&quot;])&#10;                        job.update(detailed_job)  # Merge detailed info&#10;                        time.sleep(0.5)  # Be respectful to the server&#10;&#10;                    # Apply skill filters&#10;                    haystack = &quot; &quot;.join(&#10;                        filter(None, [job.get(&quot;skills&quot;), job.get(&quot;description&quot;), job.get(&quot;title&quot;)])&#10;                    )&#10;&#10;                    if skills_config and not _check_skills_match(haystack, skills_config):&#10;                        continue&#10;&#10;                    # Apply salary filters&#10;                    cur, amount = _parse_salary(job.get(&quot;salary&quot;, &quot;&quot;))&#10;                    if (&#10;                        (cur == &quot;USD&quot; and min_usd and amount &lt; min_usd)&#10;                        or (cur == &quot;BRL&quot; and min_brl and amount &lt; min_brl)&#10;                    ):&#10;                        continue&#10;&#10;                    site_jobs.append(job)&#10;                    page_jobs_added += 1&#10;                    print(f&quot;✓ Job {len(site_jobs)}/{max_jobs} added: {job.get('title', 'Unknown title')}&quot;)&#10;&#10;                print(f&quot;Added {page_jobs_added} jobs from this page&quot;)&#10;&#10;                # Check if we should continue to next page&#10;                if len(site_jobs) &gt;= max_jobs:&#10;                    print(f&quot;Reached target of {max_jobs} jobs for {site.get('name')}&quot;)&#10;                    break&#10;&#10;                # Get next page URL if pagination is configured&#10;                if &quot;pagination&quot; in site:&#10;                    next_url = _get_next_page_url(soup, current_url, site[&quot;pagination&quot;])&#10;                    if next_url and next_url != current_url:&#10;                        current_url = next_url&#10;                        time.sleep(1)  # Be respectful between pages&#10;                    else:&#10;                        print(&quot;No more pages available&quot;)&#10;                        break&#10;                else:&#10;                    # No pagination configured, stop after first page&#10;                    break&#10;&#10;            except Exception as e:&#10;                print(f&quot;Error scraping page {current_url}: {e}&quot;)&#10;                break&#10;&#10;        jobs.extend(site_jobs)&#10;        print(f&quot;Total jobs collected from {site.get('name')}: {len(site_jobs)}&quot;)&#10;&#10;    print(f&quot;Total jobs found across all sites: {len(jobs)}&quot;)&#10;    return jobs&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    import argparse, json&#10;&#10;    parser = argparse.ArgumentParser(description=&quot;Scrape job listings&quot;)&#10;    parser.add_argument(&quot;--config&quot;, default=&quot;../config/job_config.yaml&quot;)&#10;    args = parser.parse_args()&#10;    results = scrape_jobs(args.config)&#10;    print(json.dumps(results, indent=2, ensure_ascii=False))" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/test_skills.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/test_skills.py" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&#10;import sys&#10;sys.path.append('/home/victor/webpages/curriculo/scripts')&#10;&#10;from scrape_jobs import _check_skills_match&#10;&#10;# Teste da nova funcionalidade de skills obrigatórias/opcionais&#10;def test_skills_logic():&#10;    print(&quot;=== Testando Lógica de Skills ===&quot;)&#10;    &#10;    # Configuração de exemplo&#10;    skills_config = [&#10;        {&quot;name&quot;: &quot;Python&quot;, &quot;required&quot;: True},&#10;        {&quot;name&quot;: &quot;SQL&quot;, &quot;required&quot;: True},&#10;        {&quot;name&quot;: &quot;Docker&quot;, &quot;required&quot;: False},&#10;        {&quot;name&quot;: &quot;React&quot;, &quot;required&quot;: False},&#10;        &quot;Git&quot;,  # Formato antigo (opcional)&#10;        &quot;Linux&quot;&#10;    ]&#10;    &#10;    # Teste 1: Vaga que tem skills obrigatórias&#10;    job_text1 = &quot;Desenvolvedor Python com conhecimento em SQL e bancos de dados&quot;&#10;    result1 = _check_skills_match(job_text1, skills_config)&#10;    print(f&quot;Teste 1 - Vaga com Python e SQL: {result1} ✓&quot;)&#10;    &#10;    # Teste 2: Vaga que não tem skills obrigatórias &#10;    job_text2 = &quot;Desenvolvedor JavaScript com React e Docker&quot;&#10;    result2 = _check_skills_match(job_text2, skills_config)&#10;    print(f&quot;Teste 2 - Vaga sem Python/SQL: {result2} ✗&quot;)&#10;    &#10;    # Teste 3: Vaga com apenas uma skill obrigatória&#10;    job_text3 = &quot;Programador Python para desenvolvimento web&quot;&#10;    result3 = _check_skills_match(job_text3, skills_config)&#10;    print(f&quot;Teste 3 - Vaga só com Python: {result3} ✗&quot;)&#10;    &#10;    # Teste 4: Vaga com skills obrigatórias + opcionais&#10;    job_text4 = &quot;Desenvolvedor Python e SQL com experiência em Git&quot;&#10;    result4 = _check_skills_match(job_text4, skills_config)&#10;    print(f&quot;Teste 4 - Vaga com obrigatórias + Git: {result4} ✓&quot;)&#10;    &#10;    print(&quot;\n=== Resumo ===&quot;)&#10;    print(&quot;- Para passar no filtro, a vaga DEVE conter TODAS as skills obrigatórias&quot;)&#10;    print(&quot;- Skills opcionais servem como bônus, mas não são suficientes sozinhas&quot;)&#10;    print(&quot;- Formato antigo (string simples) continua funcionando como opcional&quot;)&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    test_skills_logic()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>